{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle Titanic Baseline\n",
        "\n",
        "This notebook mirrors the baseline pipeline in `src/train.py` and produces a `submission.csv` file for the Kaggle Titanic competition.\n",
        "\n",
        "**Before you start:** place `train.csv` and `test.csv` inside the `data/` directory. The final submission file will be written to `output/submission.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and inspect the data\n",
        "\n",
        "We verify the input files exist, load them into pandas, and take a quick look at their shape and columns for easy debugging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "data_dir = Path(\"data\")\n",
        "train_path = data_dir / \"train.csv\"\n",
        "test_path = data_dir / \"test.csv\"\n",
        "\n",
        "if not train_path.exists() or not test_path.exists():\n",
        "    missing = [str(p) for p in [train_path, test_path] if not p.exists()]\n",
        "    raise FileNotFoundError(\n",
        "        \"Missing input files: \"\n",
        "        + \", \".join(missing)\n",
        "        + \". Download them from Kaggle and place them in the data/ directory.\"\n",
        "    )\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the preprocessing + model pipeline\n",
        "\n",
        "We align the feature list with the training script, split numeric vs. categorical columns, and build a preprocessing pipeline with a logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    \"Pclass\",\n",
        "    \"Sex\",\n",
        "    \"Age\",\n",
        "    \"SibSp\",\n",
        "    \"Parch\",\n",
        "    \"Fare\",\n",
        "    \"Embarked\",\n",
        "]\n",
        "\n",
        "missing_cols = [col for col in feature_cols if col not in train_df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Training data is missing columns: {missing_cols}\")\n",
        "\n",
        "missing_test_cols = [col for col in feature_cols if col not in test_df.columns]\n",
        "if missing_test_cols:\n",
        "    raise ValueError(f\"Test data is missing columns: {missing_test_cols}\")\n",
        "\n",
        "numeric_features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "categorical_features = [\"Sex\", \"Embarked\"]\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"model\", model),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "We fit the pipeline on the full training dataset, mirroring the approach in `src/train.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "x = train_df[feature_cols]\n",
        "y = train_df[\"Survived\"]\n",
        "x_test = test_df[feature_cols]\n",
        "\n",
        "pipeline.fit(x, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a submission file\n",
        "\n",
        "We run the fitted model on the test set and save the Kaggle-ready CSV to `output/submission.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "predictions = pipeline.predict(x_test)\n",
        "\n",
        "submission = pd.DataFrame(\n",
        "    {\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": predictions}\n",
        ")\n",
        "\n",
        "output_dir = Path(\"output\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_path = output_dir / \"submission.csv\"\n",
        "submission.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Saved submission file to {output_path.resolve()}\")\n",
        "submission.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}